import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import streamlit as st

sns.set()

# Introduction - Data Cleaning and Processing Steps
st.markdown("## Data Cleaning and Processing Steps:")

st.markdown(
    """Our project involves a comprehensive data pre-processing and feature engineering phase, executed using Python. This pivotal stage ensures that the dataset is refined and optimised for our subsequent analyses. Below are the key steps involved:\n\n
Data Cleaning\n\n\n\n
    __**\n\n
●	Handling Missing Data:** We identified and addressed any missing data points in our datasets. This will be achieved by employing various techniques, such as imputation or interpolation, to maintain data integrity. Additionally, we will use the fillna method to replace missing values and various statistical methods to inform our decisions.\n\n
●	**Outlier Detection and Treatment:** Outliers can significantly impact our analyses. We made use of the IQR (Interquartile Range) to detect outliers. Data points outside a defined range beyond the upper and lower quartiles were considered outliers.\n\n
●	**Duplications:** Duplicates in the dataset can lead to inaccuracies. We will utilise the duplicated and drop_duplicates methods to identify and remove duplicate records, ensuring data consistency.\n\n
●	**Modifying Data:** We will use the replace method to modify elements of the DataFrame, potentially changing data types with the astype method. Additionally, we may rename columns to enhance readability and comprehension.\n\n
●	**Applying Functions:** The apply method, in conjunction with lambda functions, will allow us to apply specific operations to the DataFrame, further enhancing data quality.\n\n
 )""")

# Introduction - Data Integration
st.markdown("### Data Integration")

st.write("""
●	**Merging Datasets:** Since our project involves the utilisation of data from different sources, we will merge these datasets into a unified and cohesive dataset. This step may require joining operations and concatenation to bring together relevant information from various files.\n\n
Here is an example of how we performed a standardisation process, in this case, a baseline shift.\n\n
**Filtering the Data:**\n
Firstly, we filtered the temperature anomaly dataset (df_temp) to focus on the period between 1880 and 1900. This is the "pre-industrial" period defined.\n\n
**Calculating the Average:**\n
After filtering, we calculated the average temperature anomalies (average) for this specified pre-industrial period.\n\n
**Shifting the Baseline:**\n
We subtracted this calculated average from all the temperature anomalies in the dataset. This shifts the baseline from the 1951-1980 period to the pre-industrial period (1880-1900). This subtraction aligns the temperature anomalies with the "pre-industrial" reference, allowing us to compare them with the target of a "1.5ºC maximum increase from the pre-industrial average."\n\n
The purpose of this baseline shift is to ensure that we’re comparing temperature anomalies consistently with the defined target. By setting the pre-industrial period as the new reference, we're aligning the analysis with the specific goal of understanding temperature changes concerning the pre-industrial era.\n\n
This is a crucial step in ensuring the relevance and accuracy of the analysis, especially when dealing with climate data that spans different reference periods. It helps to make the data more interpretable and relevant to our research objectives.\n\n
""")
# Introduction - Feature Engineering
st.markdown("### Feature Engineering")

st.write("""
●	Creation of Derived Features: In our quest to understand global climate trends, we may need to create derived features. This could include aggregating data, calculating moving averages, or other transformations to provide valuable insights into long-term climate changes.
""")

# Introduction - Data Visualization Tools:
st.markdown("### Data Visualization Tools: ")
st.write("""To analyse and visualise the data, we will employ a combination of data visualisation tools, including Seaborn, Plotly, and Microsoft Power BI. These tools offer diverse visualisation options and will enable us to present our findings effectively.\n\n
    """)

# Introduction - Project Objectives:
st.markdown("### Project Objectives: ")

st.write("""Our primary objective is to answer the following key questions:\n\n
1.	**Global Climate Trends:** We will analyse global temperature anomalies over the past 100 years and since the year 2000, allowing us to assess long-term and recent climate changes.\n\n
2.	**Geographical Analysis:** We will perform a geographical analysis of Europe, USA and China to identify regional climate variations and their correlation with emissions data, considering both the past 100 years and since 2000.
3.	**Comparison with Historical Phases:** Our project will compare the current temperature anomalies with historical climate phases, particularly focusing on the past 100 years and since the year 2000 till present. This comparison will shed light on the extent of global warming and climate disruption and potential future outlook.\n\n
\n\n
""")
# Introduction - Technical Challenges:
st.markdown("### Technical Challenges: ")
st.write("""
1.	**Data Integration:** Combining datasets from different sources and formats, such as CO2 emissions data and temperature anomalies, can be challenging. Ensuring data consistency and accuracy during integration is crucial.
2.	**Data Quality:** Cleaning and preprocessing large datasets, especially handling missing data and outliers, can be time-consuming and may require advanced data cleaning techniques.\n\n
3.	**Data Volume:** Large volumes of historical climate and emissions data may pose challenges in terms of storage, processing, and computational requirements.\n\n
4.	**Complex Analysis:** Analysing temperature anomalies, emissions data, and their correlations across various geographical regions and timeframes can be technically complex.\n\n
5.	**Data Visualization:** Effectively visualising complex climate data and trends using tools like Seaborn, Plotly, and Microsoft Power BI requires expertise in data visualisation and interpretation.\n\n""")

# Introduction - Technical Opportunities:
st.markdown("### Technical Opportunities: ")
st.write("""
1.	**Advanced Tools:** Leveraging advanced data analysis tools, such as Python, Seaborn, and Plotly, presents an opportunity to create meaningful visualisations and insights.\n\n
2.	**Machine Learning:** We “may” explore the potential of machine learning techniques to predict climate trends, identify patterns, or optimise data preprocessing.\n\n
3.	**Big Data Solutions:** If dealing with large datasets, you can explore big data technologies and cloud computing solutions to handle data storage, processing, and analysis efficiently.\n\n
4.	**Open Data Resources:** The availability of open data resources, such as NASA's climate datasets and Our World in Data, provides a valuable opportunity for comprehensive climate analysis.\n\n
5.	**Interdisciplinary Insights:** Collaborating with experts in climate science, data science, and domain-specific fields can enrich the project with diverse insights and methodologies.\n\n
""")
# Introduction - Economic Context:
st.markdown("### Economic Context:  ")
st.write("""
This project, which focuses on the analysis of global climate data and greenhouse gas emissions, has several economic implications. Some of the economic benefits of this project in regards to the information that it is helping to disseminate to companies specifically within the European Union are highlighted below. There are several hypothesised advantages for corporations acting in an environmentally friendly manner:\n\n""")

# Introduction - Economic Implications
st.markdown("### Economic Implications: ")
st.write("""
1.	**Cost Savings:** By analysing data related to greenhouse gas emissions, European companies can identify opportunities to reduce energy consumption and emissions. Implementing energy-efficient practices and technologies can lead to significant cost savings in the long run. This aligns with the economic goal of reducing operational expenses.\n\n
2.	**Regulatory Compliance:** European companies are subject to stringent environmental regulations and emissions targets. This project can help companies ensure compliance with these regulations, avoiding potential fines and legal issues that may impact their financial stability.\n\n
3.	**Reputation and Branding:** Environmental responsibility is increasingly important to consumers and investors. European companies that are environmentally friendly and transparent in their sustainability efforts can enjoy a positive brand image. This can lead to increased customer loyalty and, in turn, higher revenues.\n\n
4.	**Innovation and Competitive Advantage:** Companies that invest in sustainable practices and technologies are often at the forefront of innovation. Your project may identify innovative ways to reduce emissions and resource usage, which can provide a competitive advantage in the European market.\n\n
5.	**Access to Green Markets:** European companies that prioritise sustainability can access green markets and attract environmentally conscious customers. These markets often offer premium pricing for eco-friendly products and services, potentially increasing revenue.\n\n
6.	**Reduced Supply Chain Risks:** Companies can use data-driven insights from your project to assess and reduce environmental risks in their supply chains. This can result in better risk management and cost reduction, positively impacting their financial stability.\n\n
7.	**Investor Attraction:** Many investors are looking for socially responsible and environmentally sustainable companies to invest in. Your project's findings can attract responsible investors who see long-term financial potential in green and eco-friendly businesses.\n\n""")

# Introduction - Data Integration:
st.markdown("### Data Integration: ")
st.write("""By exploring historical temperature data and examining contemporary climate trends, we aim to contribute to scientific understanding in a field with immense scientific relevance. Our findings will provide valuable insights for climate scientists and researchers working toward a more comprehensive understanding of the effects of global climate change. The scientific aspects of our project revolve around several key elements:\n\n
**Climate Science and Data Analysis:** Our project is rooted in the field of climate science. We utilise scientific principles and methodologies to analyse extensive datasets, such as NASA's Land-Ocean Temperature Index (L-OTI). These datasets contain valuable information about temperature anomalies and deviations from the 1951-1980 means. Our analysis involves statistical methods and data visualisation techniques to uncover patterns, trends, and anomalies in global climate data.\n\n
**Understanding Climate Change:** Climate change is a critical scientific topic with far-reaching implications. Our project aims to contribute to a deeper understanding of climate change by exploring historical temperature data. We examine variations over the past century and the last three decades. By doing so, we seek to identify and quantify the effects of global warming and climate disruption.\n\n
**Scientific Relevance:** Our project is highly relevant in the field of climate science and environmental studies. It aligns with ongoing scientific research to monitor and comprehend the impacts of climate change on a global scale. By analysing data from the past and recent decades, we contribute to the broader scientific knowledge base focused on global climate trends and variability.\n\n
**Data-Driven Insights:** We leverage scientific methodologies for data preprocessing and feature engineering. This involves cleaning and enhancing the datasets to ensure accuracy and reliability. We also use data visualisation techniques to present scientific findings in a comprehensible manner.\n\n
**Contribution to Scientific Knowledge:** Our project seeks to make a meaningful contribution to scientific knowledge by providing insights into global climate changes. The analysis will reveal how different geographical areas are affected and how these changes compare to previous temperature phases. These findings add to the body of evidence supporting climate science research.\n\n
""")
# Introduction - Data Transformation:
st.markdown("### Data Transformation: ")

st.write("""
●	Temporal Alignment: Given the specific goal of assessing temperature anomalies concerning the "1.5ºC increase above pre-industrial levels," we will undertake data transformations. This will involve aligning the temporal aspect of our data to match this reference point, ensuring the relevance of our analyses.
""")











